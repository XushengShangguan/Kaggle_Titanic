{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    # Load data from CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Fill missing ages with the median age\n",
    "    df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "    \n",
    "    # Fill missing Embarked values with the mode\n",
    "    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "    \n",
    "    # Create a new feature for FamilySize (including the passenger)\n",
    "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "    \n",
    "    # Categorize FamilySize into groups (Alone, Small, Large)\n",
    "    df[\"FamilyGroup\"] = pd.cut(\n",
    "        df[\"FamilySize\"],\n",
    "        bins=[0, 1, 4, np.inf],\n",
    "        labels=[\"Alone\", \"Small\", \"Large\"]\n",
    "    )\n",
    "    \n",
    "    # Create a binary feature where 1 indicates the passenger is alone\n",
    "    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n",
    "    \n",
    "    # Combine Pclass and FamilySize to capture potential interaction effects\n",
    "    df[\"Pclass_FamilySize\"] = df[\"Pclass\"] * df[\"FamilySize\"]\n",
    "    \n",
    "    # Extract titles from the Name using a regular expression\n",
    "    df[\"Title\"] = df[\"Name\"].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "    # Replace some of the less common or similar titles\n",
    "    df[\"Title\"] = df[\"Title\"].replace([\"Rev\", \"Dr\", \"Col\", \"Major\"], \"Rare\")\n",
    "    df[\"Title\"] = df[\"Title\"].replace([\"Mlle\", \"Ms\"], \"Miss\")\n",
    "    df[\"Title\"] = df[\"Title\"].replace(\"Mme\", \"Mrs\")\n",
    "    \n",
    "    # Map the titles to numerical values\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    df[\"Title\"] = df[\"Title\"].map(title_mapping)\n",
    "    \n",
    "    # Convert Sex into a binary variable: 0 for male and 1 for female\n",
    "    df[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "    \n",
    "    # One-hot encode the Embarked column\n",
    "    df = pd.get_dummies(df, columns=[\"Embarked\"], prefix=\"Embarked\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = preprocess_data('train.csv')\n",
    "df_test = preprocess_data('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[['Pclass', 'Sex']].values\n",
    "y_train = df_train[['Survived']].values\n",
    "\n",
    "X_test = df_test[['Pclass', 'Sex']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max Depth = 2\n",
      "  Precision: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0]\n",
      "\n",
      "Max Depth = 3\n",
      "  Precision: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0]\n",
      "\n",
      "Max Depth = 4\n",
      "  Precision: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0]\n",
      "\n",
      "Max Depth = 5\n",
      "  Precision: [0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Using several depths of decision tree to predict\n",
    "depths = [2, 3, 4, 5]\n",
    "for depth in depths:\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=depth, criterion='entropy', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_pred = clf.predict(X_test).reshape(-1)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nMax Depth = {depth}\")\n",
    "    print(f\"  Precision: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = preprocess_data('train.csv')\n",
    "\n",
    "X = df_train['Age']\n",
    "y = df_train[['Survived']].values\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/justin/Desktop/CS4100/Kaggle_Titanic/prediction.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/justin/Desktop/CS4100/Kaggle_Titanic/prediction.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/justin/Desktop/CS4100/Kaggle_Titanic/prediction.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/justin/Desktop/CS4100/Kaggle_Titanic/prediction.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m weights, bias \u001b[39m=\u001b[39m logistic_regression_l1(X_train, y_train, lr\u001b[39m=\u001b[39;49mlearning_rate, epochs\u001b[39m=\u001b[39;49mepochs, lambda_l1\u001b[39m=\u001b[39;49mlambda_l1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/justin/Desktop/CS4100/Kaggle_Titanic/prediction.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m y_test_pred \u001b[39m=\u001b[39m predict(X_test, weights, bias)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/justin/Desktop/CS4100/Kaggle_Titanic/prediction.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m accuracy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(y_test_pred \u001b[39m==\u001b[39m y_test)\n",
      "\u001b[1;32m/Users/justin/Desktop/CS4100/Kaggle_Titanic/prediction.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justin/Desktop/CS4100/Kaggle_Titanic/prediction.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mlogistic_regression_l1\u001b[39m(X, y, lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, lambda_l1\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/justin/Desktop/CS4100/Kaggle_Titanic/prediction.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     m, n \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justin/Desktop/CS4100/Kaggle_Titanic/prediction.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(n)  \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/justin/Desktop/CS4100/Kaggle_Titanic/prediction.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     bias \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression_l1(X, y, lr=0.01, epochs=1000, lambda_l1=0.1):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)  \n",
    "    bias = 0 \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        z = np.dot(X, weights) + bias\n",
    "        predictions = sigmoid(z)\n",
    "        \n",
    "        error = predictions - y\n",
    "        dw = (1 / m) * np.dot(X.T, error) + lambda_l1 * np.sign(weights)  # L1 gradient\n",
    "        db = (1 / m) * np.sum(error)\n",
    "        \n",
    "        weights -= lr * dw\n",
    "        bias -= lr * db\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "def predict(X, weights, bias):\n",
    "    z = np.dot(X, weights) + bias\n",
    "    probabilities = sigmoid(z)\n",
    "    return np.where(probabilities >= 0.5, 1, 0)\n",
    "\n",
    "lambda_l1 = 0.1 \n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "weights, bias = logistic_regression_l1(X_train, y_train, lr=learning_rate, epochs=epochs, lambda_l1=lambda_l1)\n",
    "\n",
    "y_test_pred = predict(X_test, weights, bias)\n",
    "\n",
    "accuracy = np.mean(y_test_pred == y_test)\n",
    "print(f\"Classification accuracy on the test set: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
